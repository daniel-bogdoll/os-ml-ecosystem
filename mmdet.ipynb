{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# MM Development Suite"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The MM development suite is a general toolbox (based on pytorch), which allows you to easily build and train models based on existing methods and test them on different datasets. Therefore, you do not need to worry about building dataloaders or augmentations yourself and having to worry about countless bugs.\n",
    "The original repositories already contains a large number of approaches, though, a lot of methods build their extensions separately. The toolbox can easily be extended with custom modules.\n",
    "\n",
    "Currently, there are basically two \"worlds\" of MM."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Old MM World\n",
    "While the development of the old mm world has already ended around two years ago, most of the current SotA approaches are still relying on it.\n",
    "Famous examples are:\n",
    "- BEVFormer (https://github.com/fundamentalvision/BEVFormer)\n",
    "- UniAD (https://github.com/OpenDriveLab/UniAD)\n",
    "- STREAM-PETR (https://github.com/exiawsh/StreamPETR)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"imgs/mm_old.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## New MM World"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In 2023, the MM suite got a complete rework and refactoring. While the code may look similar, they are not compatible which each other. In the new version they introduced MMEngine, which now contains a lot of the original MMCV functions. MMEngine is responsible for general things as training configurations, optimizer and co., whereas MMCV now solely handles detection foundations such as Deformable Attention kernels."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<img src=\"imgs/mm_new.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "NOTE: Both MMDet and MMDet3D are not actively developed at the moment (the last commit was was at the beginning of 2024)."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Installation\n",
    "The installation the most recent versions are explained here in detail:\n",
    "- MMDET (https://mmdetection.readthedocs.io/en/latest/get_started.html)\n",
    "- MMDet3D (https://mmdetection3d.readthedocs.io/en/latest/get_started.html)\n",
    "\n",
    "The environment has many dependencies, which is why I recommend using a docker container or devcontainer. An exemplary Dockerfile for MMDet and MMDet3D can be found in this repo. These containers are by default set to developer mode, which increases build time to more than 30 Minutes. For quick testing, just comment/uncomment the highlighted section in the respective Dockerfile."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Example config\n",
    "In MMDET/3D everything revolves around configs. These configs are basically the template that is used in order to build the final model including the training and evaluation process. These configs are defined as dictionaries, and the type parameter usually refers to the underlying class and the following parameters correspond to the initialization parameters.\n",
    "In the following, we showcase how a config of the \"new world\" might look using DETR as an example, which is trained on MS COCO."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Model"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T19:44:55.841679Z",
     "start_time": "2025-03-19T19:44:55.808066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = dict(\n",
    "    type='DETR',\n",
    "    num_queries=100,\n",
    "    data_preprocessor=dict(\n",
    "        type='DetDataPreprocessor',\n",
    "        mean=[123.675, 116.28, 103.53],\n",
    "        std=[58.395, 57.12, 57.375],\n",
    "        bgr_to_rgb=True,\n",
    "        pad_size_divisor=1),\n",
    "    backbone=dict(\n",
    "        type='ResNet',\n",
    "        depth=50,\n",
    "        num_stages=4,\n",
    "        out_indices=(3, ),\n",
    "        frozen_stages=1,\n",
    "        norm_cfg=dict(type='BN', requires_grad=False),\n",
    "        norm_eval=True,\n",
    "        style='pytorch',\n",
    "        init_cfg=dict(type='Pretrained', checkpoint='torchvision://resnet50')),\n",
    "    neck=dict(\n",
    "        type='ChannelMapper',\n",
    "        in_channels=[2048],\n",
    "        kernel_size=1,\n",
    "        out_channels=256,\n",
    "        act_cfg=None,\n",
    "        norm_cfg=None,\n",
    "        num_outs=1),\n",
    "    encoder=dict(  # DetrTransformerEncoder\n",
    "        num_layers=6,\n",
    "        layer_cfg=dict(  # DetrTransformerEncoderLayer\n",
    "            self_attn_cfg=dict(  # MultiheadAttention\n",
    "                embed_dims=256,\n",
    "                num_heads=8,\n",
    "                dropout=0.1,\n",
    "                batch_first=True),\n",
    "            ffn_cfg=dict(\n",
    "                embed_dims=256,\n",
    "                feedforward_channels=2048,\n",
    "                num_fcs=2,\n",
    "                ffn_drop=0.1,\n",
    "                act_cfg=dict(type='ReLU', inplace=True)))),\n",
    "    decoder=dict(  # DetrTransformerDecoder\n",
    "        num_layers=6,\n",
    "        layer_cfg=dict(  # DetrTransformerDecoderLayer\n",
    "            self_attn_cfg=dict(  # MultiheadAttention\n",
    "                embed_dims=256,\n",
    "                num_heads=8,\n",
    "                dropout=0.1,\n",
    "                batch_first=True),\n",
    "            cross_attn_cfg=dict(  # MultiheadAttention\n",
    "                embed_dims=256,\n",
    "                num_heads=8,\n",
    "                dropout=0.1,\n",
    "                batch_first=True),\n",
    "            ffn_cfg=dict(\n",
    "                embed_dims=256,\n",
    "                feedforward_channels=2048,\n",
    "                num_fcs=2,\n",
    "                ffn_drop=0.1,\n",
    "                act_cfg=dict(type='ReLU', inplace=True))),\n",
    "        return_intermediate=True),\n",
    "    positional_encoding=dict(num_feats=128, normalize=True),\n",
    "    bbox_head=dict(\n",
    "        type='DETRHead',\n",
    "        num_classes=80,\n",
    "        embed_dims=256,\n",
    "        loss_cls=dict(\n",
    "            type='CrossEntropyLoss',\n",
    "            bg_cls_weight=0.1,\n",
    "            use_sigmoid=False,\n",
    "            loss_weight=1.0,\n",
    "            class_weight=1.0),\n",
    "        loss_bbox=dict(type='L1Loss', loss_weight=5.0),\n",
    "        loss_iou=dict(type='GIoULoss', loss_weight=2.0)),\n",
    "    # training and testing settings\n",
    "    train_cfg=dict(\n",
    "        assigner=dict(\n",
    "            type='HungarianAssigner',\n",
    "            match_costs=[\n",
    "                dict(type='ClassificationCost', weight=1.),\n",
    "                dict(type='BBoxL1Cost', weight=5.0, box_format='xywh'),\n",
    "                dict(type='IoUCost', iou_mode='giou', weight=2.0)\n",
    "            ])),\n",
    "    test_cfg=dict(max_per_img=100))"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataloader & Dataset"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(\n",
    "        type='RandomChoice',\n",
    "        transforms=[[\n",
    "            dict(\n",
    "                type='RandomChoiceResize',\n",
    "                scales=[(480, 1333), (512, 1333), (544, 1333), (576, 1333),\n",
    "                        (608, 1333), (640, 1333), (672, 1333), (704, 1333),\n",
    "                        (736, 1333), (768, 1333), (800, 1333)],\n",
    "                keep_ratio=True)\n",
    "        ],\n",
    "                    [\n",
    "                        dict(\n",
    "                            type='RandomChoiceResize',\n",
    "                            scales=[(400, 1333), (500, 1333), (600, 1333)],\n",
    "                            keep_ratio=True),\n",
    "                        dict(\n",
    "                            type='RandomCrop',\n",
    "                            crop_type='absolute_range',\n",
    "                            crop_size=(384, 600),\n",
    "                            allow_negative_crop=True),\n",
    "                        dict(\n",
    "                            type='RandomChoiceResize',\n",
    "                            scales=[(480, 1333), (512, 1333), (544, 1333),\n",
    "                                    (576, 1333), (608, 1333), (640, 1333),\n",
    "                                    (672, 1333), (704, 1333), (736, 1333),\n",
    "                                    (768, 1333), (800, 1333)],\n",
    "                            keep_ratio=True)\n",
    "                    ]]),\n",
    "    dict(type='PackDetInputs')\n",
    "]\n",
    "test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=None),\n",
    "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),\n",
    "    # If you don't have a gt annotation, delete the pipeline\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape',\n",
    "                   'scale_factor'))\n",
    "]\n",
    "\n",
    "dataset_type = 'CocoDataset'\n",
    "data_root = 'data/coco/'\n",
    "\n",
    "backend_args = None\n",
    "\n",
    "train_dataloader = dict(\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
    "    batch_sampler=dict(type='AspectRatioBatchSampler'),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        ann_file='annotations/instances_train2017.json',\n",
    "        data_prefix=dict(img='train2017/'),\n",
    "        filter_cfg=dict(filter_empty_gt=True, min_size=32),\n",
    "        pipeline=train_pipeline,\n",
    "        backend_args=backend_args))\n",
    "val_dataloader = dict(\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True,\n",
    "    drop_last=False,\n",
    "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
    "    dataset=dict(\n",
    "        type=dataset_type,\n",
    "        data_root=data_root,\n",
    "        ann_file='annotations/instances_val2017.json',\n",
    "        data_prefix=dict(img='val2017/'),\n",
    "        test_mode=True,\n",
    "        pipeline=test_pipeline,\n",
    "        backend_args=backend_args))\n",
    "test_dataloader = val_dataloader"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Evaluation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_evaluator = dict(\n",
    "    type='CocoMetric',\n",
    "    ann_file=data_root + 'annotations/instances_val2017.json',\n",
    "    metric='bbox',\n",
    "    format_only=False,\n",
    "    backend_args=backend_args)\n",
    "test_evaluator = val_evaluator"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Logging and Visualization"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "default_scope = 'mmdet'\n",
    "\n",
    "default_hooks = dict(\n",
    "    timer=dict(type='IterTimerHook'),\n",
    "    logger=dict(type='LoggerHook', interval=50),\n",
    "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
    "    checkpoint=dict(type='CheckpointHook', interval=1),\n",
    "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
    "    visualization=dict(type='DetVisualizationHook'))\n",
    "\n",
    "env_cfg = dict(\n",
    "    cudnn_benchmark=False,\n",
    "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
    "    dist_cfg=dict(backend='nccl'),\n",
    ")\n",
    "\n",
    "vis_backends = [dict(type='LocalVisBackend')]\n",
    "visualizer = dict(\n",
    "    type='DetLocalVisualizer', vis_backends=vis_backends, name='visualizer')\n",
    "log_processor = dict(type='LogProcessor', window_size=50, by_epoch=True)\n",
    "\n",
    "log_level = 'INFO'\n",
    "load_from = None"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### General Settings"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# optimizer\n",
    "optim_wrapper = dict(\n",
    "    type='OptimWrapper',\n",
    "    optimizer=dict(type='AdamW', lr=0.0001, weight_decay=0.0001),\n",
    "    clip_grad=dict(max_norm=0.1, norm_type=2),\n",
    "    paramwise_cfg=dict(\n",
    "        custom_keys={'backbone': dict(lr_mult=0.1, decay_mult=1.0)}))\n",
    "\n",
    "# learning policy\n",
    "max_epochs = 150\n",
    "train_cfg = dict(\n",
    "    type='EpochBasedTrainLoop', max_epochs=max_epochs, val_interval=1)\n",
    "val_cfg = dict(type='ValLoop')\n",
    "test_cfg = dict(type='TestLoop')\n",
    "\n",
    "param_scheduler = [\n",
    "    dict(\n",
    "        type='MultiStepLR',\n",
    "        begin=0,\n",
    "        end=max_epochs,\n",
    "        by_epoch=True,\n",
    "        milestones=[100],\n",
    "        gamma=0.1)\n",
    "]\n",
    "\n",
    "# NOTE: `auto_scale_lr` is for automatically scaling LR,\n",
    "# USER SHOULD NOT CHANGE ITS VALUES.\n",
    "# base_batch_size = (8 GPUs) x (2 samples per GPU)\n",
    "auto_scale_lr = dict(base_batch_size=16)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "After having your config finished, training your model is easily done using already provided scripts. Either use the python script in MMDET/3D from \"tools/train.py\" or the bash script \"tools/dist_train.sh\". for multi-gpu training. Standard usage looks like this:\n",
    "```bash\n",
    "python tools/train.py my_config.py\n",
    "```\n",
    "or\n",
    "```bash\n",
    "./tools/dist_train.sh my_config.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
